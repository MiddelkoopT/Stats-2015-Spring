# Engineering Statistics
IMSE 4410 Spring 2015. 
Copyright 2013-2015 by Timothy Middelkoop License CC by SA 3.0

## Linear Regression (Chapter 11, Week 10)

Linear regression is developing a model to explain the variation in sample data.  Before we assumed a homogeneous population of some distribution and asked questions about the parameters of this population (point estimation, confidence interval, and hypothesis test).

Understand the assumptions behind regression.  Regression only works over the range of observed values, in some sense it is just interpolation.

### Simple Linear Regression (Chapter 11-1)

* One independent variable (regressor, x), 
* One dependent variable (regressand, response varable, y), 
* Continuous variables, linear relationship. (y=mx+b)
* $y=mx+b$ -> $E(Y|x) = \mu_{Y|x} = \beta_0+\beta_1 x$ ; $Y=\hat\beta_0+\hat\beta_1 x + \epsilon$ ; $V(Y|x) = \sigma^2$

$\epsilon$ is the error term, mean 0, unknown variance $\sigma^2$ and uncorrelated.

Finding $\beta$: Least squares. (Equation 11-4 6e)

Minimize: 
$$ L=\sum \epsilon^2=\sum (y-\beta_0-\beta_1 x_1)^2$$
 
or in R vector form notation
```{r eval=FALSE}
L=sum(e^2)=sum((y-b0-b1 *x1)^2)
```

Solve directly to get (Equation 11-6 5e): bh0 ($\hat\beta_0$) is the estimate.

**Least squares normal equations**:

$$n\hat\beta_0 + \hat\beta_1 \sum_{i=1}^n x = \sum_{i=1}^n y$$
$$\hat\beta_0 \sum{i=1}^n x + \hat\beta_1 \sum_{i=1}^n x^2 = \sum_{i=1}^n x\cdot y $$

```{r eval=FALSE}
n*bh0+bh1*sum(x)=sum(y) ;  bh0*sum(x)+bh1(sum(x^2))=sum(x*y)
```

Solving this we get the **least square estimates**:

$$\hat\beta_0 = \bar y - \hat\beta_1 \cdot \bar x$$
$$\hat\beta_1 = {S_{xy} \over S_{xx} }$$
$$S_{xx} = \sum_{i=1}^n (x-\bar x)^2$$
$$S_{xy} = \sum_{i=1}^n (y-\bar y)(x-\bar x)$$

```{r eval=FALSE}
bh0 <- yb-bh1*xb ; yb <- mean(y) ; xb <- mean(y)
bh1 <- sxy/sxx ; sxy <- sum((y-yb)*(x-xb)); sxx <- sum((x-xb)^2)
```

### Example 11-1
Load the data:
```{r}
ch11 <- read.csv("data/5e/ch11.csv",header=TRUE)
nrow(ch11)

d <- na.omit(data.frame(x=ch11$Example11.1Level,y=ch11$Example11.1Purity)) ; n <- nrow(d) ; n # [1] 20

# Plot, it's a line.
plot(y~x,d)

```

Note the presentation in the text is backwards, unravel and use an example (11-1 5e/6e)


```{r}
y <- d$y ; x <-d$x ; n <- nrow(d) ; n # [1] 20
yb <- mean(y) ; yb # [1] 92.1605 
xb <- mean(x) ; xb # [1] 1.196

sxx <- sum((x-xb)^2) ; sxx # [1] 0.68088
sxy <- sum((y-yb)*(x-xb)) ; sxy # [1] 10.17744

bh1 <- sxy/sxx ; bh1 # [1] 14.94748
bh0 <- yb-bh1*xb ; bh0 # [1] 74.28331

# Plot with regression model
plot(y~x,d) ; abline(bh0,bh1)

# our fitted model is 
yh <- bh0+bh1*x ; yh
```

**Estimating $\sigma^2$**

The estimate of $\sigma^2$ is $\hat\sigma^2$ (sigmahsq). The residuals ($\epsilon=y-\hat y$) is the unexplained variance and is used to estimate $\sigma^2$. The error sum of squares (sse) is calculated as follows.  

```{r}
sse <- sum((y-yh)^2) ; sse # [1] 21.24982
sigmahsq <- sse/(n-2) ; sigmahsq # [1] 1.180545
```

**Using R directly**

```{r}

m <- lm(y~x,d) ; summary(m)

names(m)
names(summary(m))

## yh
m$fitted.values

## standard error
sqrt(sigmahsq) # [1] 1.086529

```

## References
* Linear Regression http://www3.nd.edu/~steve/Rcourse/Lecture8v1.pdf