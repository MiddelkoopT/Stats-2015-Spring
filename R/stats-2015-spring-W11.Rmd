---
output: pdf_document
---
# Engineering Statistics
IMSE 4410 Spring 2015. 
Copyright 2013-2015 by Timothy Middelkoop License CC by SA 3.0

## Linear Regression (Chapter 11, Week 11)

```{r results='hide'}
ch11 <- read.csv("data/5e/ch11.csv",header=TRUE)
nrow(ch11)

d <- na.omit(data.frame(x=ch11$Example11.1Level,y=ch11$Example11.1Purity)) ; n <- nrow(d) ; n # [1] 20

y <- d$y ; x <-d$x ; n <- nrow(d) ; n # [1] 20
yb <- mean(y) ; yb # [1] 92.1605 
xb <- mean(x) ; xb # [1] 1.196
sxx <- sum((x-xb)^2) ; sxx # [1] 0.68088
sxy <- sum((y-yb)*(x-xb)) ; sxy # [1] 10.17744
bh1 <- sxy/sxx ; bh1 # [1] 14.94748
bh0 <- yb-bh1*xb ; bh0 # [1] 74.28331
yh <- bh0+bh1*x ; yh
```

### Chapter 11-4: Analysis of Variance

This entire class is about accounting for variance.

Where did it go?  How do we see it?

We start at the beginning, total variance, which is the distance between an 
observation and the mean of the sample, then explain it by the regression
model and the natural error term (NID!).

* $SS_T$ (sst), total corrected sum of squares;
* $SS_R$ (ssr), regression sum of squares;
* $SS_E$ (sse), error sum of squares;
* $SS_T = SS_R + SS_T$ (sst = ssr + sse)

Know theses, understand theses, you should be able to think it out on a test.


```{r}
ssr <- sum((yh-yb)^2) ; ssr # [1] 152.1271
sse <- sum((y-yh)^2) ; sse # [1] 21.24982


sst <- sum((y-yb)^2) ; sst # [1] 173.3769
ssr + sse # [1] 173.3769
```

Note we calculate this directly, not using the "old" way in the text.
Exams will be structured such that it is reasonable to take the "R" approach

From this we can test if $\hat\beta_1$ is significant.
Please read section 11-4.2 to understand where this came from.

* H0:$\hat\beta_1=0$, H1:$\hat\beta_1\ne0$
* Test Statistic: $F_0={SS_R/1 \over SS_E/(n-2)} = {MS_R \over MS_E}$, follows the F distribution with 1,n-2 degress of freedom (DOF)
* Reject H0 if $f_0 > f_{\alpha,1,n-2}$, or p-value < $\alpha$

```{r}
## msr and mse are mean squares and is the sum of square divided by the DOF.
msr <- ssr/1 ; msr # [1] 152.1271
mse <- sse/(n-2) ; mse # [1] 1.180545

## f0
f0 <- msr/mse ; f0 # [1] 128.8617

## p-value
pf(f0,1,n-2,lower.tail=FALSE) # [1] 1.227314e-09
```

We reject H0 concluding that $\hat\beta_1$ is significant with a p-value of very small.

It can be shown that the t-statistic is the same of the f-statistic.
